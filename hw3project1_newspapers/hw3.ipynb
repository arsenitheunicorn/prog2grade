{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base():\n",
    "    directory = 'Республика Башкортостан'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    table = directory + os.sep + 'metatab.csv'\n",
    "    header = 'path\\tauthor\\theader\\tcreated\\t' \\\n",
    "            'sphere\\ttopic\\tstyle\\taudience_age\\t' \\\n",
    "            'audience_level\\taudience_size\\tsource\\t' \\\n",
    "            'publication\\tpubl_year\\tmedium\\t' \\\n",
    "            'country\\tregion\\tlanguage\\n'\n",
    "    with open(table, 'w', encoding = 'utf-8') as csv:\n",
    "        csv.write(header)\n",
    "    return directory, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloaden(link):\n",
    "    time.sleep(1)\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    req = urllib.request.Request(link, headers={'User-Agent': user_agent})\n",
    "    with urllib.request.urlopen(req) as resp:\n",
    "        try:\n",
    "            html = resp.read().decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            html = resp.read().decode('utf-16')\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_archive(directory):\n",
    "    hm_page = 'https://resbash.ru'\n",
    "    hm_page_html = downloaden(hm_page)\n",
    "    pattern = '<a href=\"([^\"]+)\">Архив газеты'\n",
    "    arch_page = re.search(pattern, hm_page_html).group(1)\n",
    "    arch_url = hm_page + arch_page\n",
    "    arch_html = downloaden(arch_url)\n",
    "    pattern = 'Архив газеты!(.*?)\\n'\n",
    "    arch_links = re.search(pattern, arch_html).group(1)\n",
    "    arch_list = re.findall(\"href='[^']+'\", arch_links)\n",
    "    links_list = []\n",
    "    for link in arch_list:\n",
    "        link = re.sub(\"href=\", '', link)\n",
    "        link = re.sub(\"'\", '', link)\n",
    "        link = hm_page + link\n",
    "        links_list.append(link)\n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_month(fold_link):\n",
    "    splt = fold_link.split('/')\n",
    "    path = splt[len(splt)-1] + os.sep + splt[len(splt)-2]\n",
    "    html = downloaden(fold_link)\n",
    "    pattern = \"<a href='[^']+'[^>]*>№\"\n",
    "    href_list = re.findall(pattern, html)\n",
    "    issues_links = []\n",
    "    for href in href_list:\n",
    "        link = re.search(\"href='([^']+)'\", href).group(1)\n",
    "        lnk = r\"https://resbash.ru\" + link\n",
    "        issues_links.append(lnk)\n",
    "    return path, issues_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir_tree(path, directory, root):\n",
    "    p_dir = directory + os.sep + root + os.sep + path\n",
    "    if not os.path.exists(p_dir):\n",
    "        os.makedirs(p_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_issue(issue_link):\n",
    "    html = downloaden(issue_link)\n",
    "    pattern = \"<h2><a href='[^']+'\"\n",
    "    stat_raw = re.findall(pattern, html)\n",
    "    stat_list = []\n",
    "    for href in stat_raw:\n",
    "        link = re.search(\"href='([^']+)'\", href).group(1)\n",
    "        link = r'https://resbash.ru' + link\n",
    "        stat_list.append(link)\n",
    "    return stat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(html):\n",
    "    author_p = '>Автор:&nbsp;([А-Яа-яЁё -]+)[,<]'\n",
    "    if re.search(author_p, html, re.DOTALL):\n",
    "        author = re.search(author_p, html, re.DOTALL).group(1)\n",
    "    else:\n",
    "        author = 'None'\n",
    "    created_p = \"Опубликовано: (\\d\\d\\.\\d\\d.\\d\\d\\d\\d)\"\n",
    "    if re.search(created_p, html, re.DOTALL):\n",
    "        created = re.search(created_p, html, re.DOTALL).group(1)\n",
    "        p_year = created[len(created)-4:len(created)]\n",
    "    else:\n",
    "        created = 'None'\n",
    "        p_year = 'None'\n",
    "    topic_p = '<div class=\"div_anons_news\">Статьи рубрики <a[^>]+>([^<]+)<'\n",
    "    if re.search(topic_p, html, re.DOTALL):\n",
    "        topic = re.search(topic_p, html, re.DOTALL).group(1)\n",
    "    else:\n",
    "        topic = 'None'\n",
    "    return author, created, p_year, topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_art(directory, html, path, aut, cre, top, s_link):\n",
    "    tit_patt1 = '<title>[^-] - ([^<]*)<'\n",
    "    tit_patt2 = '<title>([^<]*)<'\n",
    "    s = os.sep\n",
    "    c = 0\n",
    "    if re.search(tit_patt1, html, re.DOTALL):\n",
    "        title = re.search(tit_patt1, html, re.DOTALL).group(1)\n",
    "        title = re.sub('[^А-Яа-яЁёCc]', '', title)\n",
    "        file_path = directory + s + 'plain' + s + path + s + title + '.txt'\n",
    "    elif re.search(tit_patt2, html, re.DOTALL):\n",
    "        title = re.search(tit_patt2, html, re.DOTALL).group(1)\n",
    "        title = re.sub('[^А-Яа-яЁёCc]', '', title)\n",
    "        file_path = directory + s + 'plain' + s + path + s + title + '.txt'\n",
    "    else:\n",
    "        title = 'None'\n",
    "        c += 1\n",
    "        ps_t = title + str(c)\n",
    "        file_path = directory + s + 'plain' + s + path + s + ps_t + '.txt'\n",
    "    m1 = '@au ' + aut\n",
    "    m2 = '@ti ' + title\n",
    "    m3 = '@da ' + cre\n",
    "    m4 = '@topic ' + top\n",
    "    m5 = '@url ' + s_link\n",
    "    meta = [m1, m2, m3, m4, m5]    \n",
    "    with open(file_path, 'w', encoding='utf-8') as art_file:\n",
    "        art_file.write('\\n'.join(meta))\n",
    "        art_file.write('\\n')\n",
    "    x_sub = \"<div style='padding:10px 20px;font-size:13px;'>\"\n",
    "    y_sub = \"</div\"\n",
    "    x = html.find(x_sub)\n",
    "    y = html.find(y_sub, x)\n",
    "    art_text = html[x+len(x_sub):y]\n",
    "    soup = BeautifulSoup(art_text, 'html.parser')\n",
    "    with open(file_path, 'a', encoding='utf-8') as art_file:\n",
    "        art_file.write(soup.get_text())\n",
    "    return title, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table(table, m_tuple):\n",
    "    row = '%s\\t%s\\t%s\\t%s\\tпублицистика\\t%s\\t' \\\n",
    "        'нейтральный\\tн-возраст\\tн-уровень\\tреспубл' \\\n",
    "        'иканская\\t%s\\tРеспублика Башкортостан\\t%s' \\\n",
    "        '\\tгазета\\tРоссия\\tБашкортостан\\tru\\n'\n",
    "    with open(table, 'a', encoding='utf-8') as m_tab:\n",
    "        m_tab.write(row % m_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_main(s_link, table, d, path):\n",
    "    html = downloaden(s_link)\n",
    "    author, created, p_year, topic = get_info(html)\n",
    "    title, f_path = txt_art(d, html, path, author, created, topic, s_link)\n",
    "    m_tuple = (f_path, author, title, created, topic, s_link, p_year)\n",
    "    update_table(table, m_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling(directory, table):\n",
    "    month_list = find_archive(directory)\n",
    "    if len(month_list):\n",
    "        for mont_link in month_list:\n",
    "            path, iss_list = every_month(mont_link)\n",
    "            make_dir_tree(path, directory, 'plain')\n",
    "            for iss_link in iss_list:\n",
    "                stat_list = every_issue(iss_link)\n",
    "                if len(stat_list) > 0:\n",
    "                    for s_link in stat_list:\n",
    "                        article_main(s_link, table, directory, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain_search(directory):\n",
    "    root = directory + os.sep + 'plain'\n",
    "    months_list = []\n",
    "    year_list = os.listdir(path=root)\n",
    "    for year in year_list:\n",
    "        y = root + os.sep + year\n",
    "        m_list = os.listdir(path=y)\n",
    "        for m in m_list:\n",
    "            month_path = y + os.sep + m\n",
    "            months_list.append(month_path)\n",
    "    return months_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "файл mystem.exe я положил в ту же папку, что и файл с кодом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_visit(list_of_dirs):\n",
    "    for path in list_of_dirs:\n",
    "        p_xml = re.sub('plain', 'mystem-xml', path)\n",
    "        p_pl = re.sub('plain', 'mystem-plain', path)\n",
    "        if not os.path.exists(p_xml):\n",
    "            os.makedirs(p_xml)\n",
    "        if not os.path.exists(p_pl):\n",
    "            os.makedirs(p_pl)\n",
    "        s_pl = \"mystem.exe -cligd --eng-gr \" \n",
    "        s_xml = s_pl + '--format xml '\n",
    "        for fl in os.listdir(path):\n",
    "            inp_name = path + os.sep + fl\n",
    "            f = open(inp_name, 'r', encoding='utf-8')\n",
    "            info = f.read()\n",
    "            f.close()\n",
    "            f2 = open('input.txt', 'w', encoding='utf-8')\n",
    "            f2.write(info)\n",
    "            f2.close()\n",
    "            txt_command = s_pl + ' input.txt output.txt'\n",
    "            os.system(txt_command)\n",
    "            with open('output.txt', 'r', encoding='utf-8') as f:\n",
    "                mstxt = f.read()\n",
    "            name = p_pl + os.sep + fl\n",
    "            with open(name, 'w', encoding='utf-8') as f:\n",
    "                f.write(mstxt)\n",
    "            xml_command = s_xml + ' input.txt output.xml'\n",
    "            os.system(xml_command)\n",
    "            with open('output.xml', 'r', encoding='utf-8') as f:\n",
    "                msxml = f.read()\n",
    "            flx = re.sub('txt', 'xml', fl)\n",
    "            name = p_xml + os.sep + flx\n",
    "            with open(name, 'w', encoding='utf-8') as f:\n",
    "                f.write(msxml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    directory, table = create_base()\n",
    "    crawling(directory, table)\n",
    "    list_of_dirs = plain_search(directory)\n",
    "    fold_visit(list_of_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
